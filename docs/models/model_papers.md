# Краткий обзор статей, решающих похожую проблему

| Название пейпера                                                                              | Краткое описание                                                                                                                                                                                                                   | Ссылка на него                                                                                    |
|-----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|
| Answering Legal Questions by Learning Neural Attentive Text Representation                    | Придумали новую архитектуру: Sentence Encoder каждое query превращает в вектор, Sentence + Paragraph encoder превращают каждый закон в один вектор. Чтобы бороться с дисбалансом, на каждый позитивную пару семплят k негативных.  | [link](https://aclanthology.org/2020.coling-main.86.pdf)                                          |
| BM25  / TF-IDF                                                                                | Самый простой классификатор, основанный на статистиках слов. Query/article - relevant/non-relevant                                                                                                                                 |                                                                                                   |
| BERT                                                                                          | Более сложный классификатор, основанный на dl. Query/article - relevant/non-relevant                                                                                                                                               |                                                                                                   |
| A Statutory Article Retrieval Dataset in French                                               | Тренировали 2 RoBERTa-encoder: один для  query, другой для article. Использовали dot-product/cosine в качестве оценки similarity.      Можно попробовать брать один общий энкодер или 2 разных.   Быстро и по метрикам как в BERT. | [link](https://arxiv.org/pdf/2108.11792v2.pdf) [repo](https://github.com/maastrichtlawtech/bsard) |
| ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT | Тренировали 2 BERT-encoder + max sim loss.  Отличается от предыдущей статьи только loss.                                                                                                                                           |                                                                                                   |
